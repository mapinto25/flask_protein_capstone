<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>EINSTEIN - Documentation</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
<script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
<style>
    .bs-example{
        margin: 20px;
    }
</style>
</head>
<body>

<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container">
        <div class="collapse navbar-collapse" id="navbarText">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item active">
                  <a class="nav-link" href="/">Home</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="/documentation">Documentation</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="/bert">Bert Model</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="/visualization">Training Data</a>
                </li>
              </ul>
        </div>
    </div>
</nav>    

 <div class="jumbotron text-center" style="margin-bottom:0">
    <h1>Documentation</h1>
</div>

<div class="container">
    <div class="bs-example">
        <div class="accordion" id="accordionExample">
            <div class="card">
                <div class="card-header" id="headingOne">
                    <h2 class="mb-0">
                        <button type="button" class="btn btn-link" data-toggle="collapse" data-target="#collapseOne">1. Getting Started</button>									
                    </h2>
                </div>
                <div id="collapseOne" class="collapse show" aria-labelledby="headingOne" data-parent="#accordionExample">
                    <div class="card-body">
                        <p><h3>Welcome to EINSTEIN! (Enzyme classIficatioN uSing proTein EmbeddINgs)</h3>
                        <br>
                        Provide us with your protein data, and of the enzymes included in your dataset, our downstream models will predict the respective enzyme classification and provide probability confidence per prediction.</br>
                        <br>
                        Our method leverages Bidirectional Encoder Representations from Transformer models to take protein strings as input, and output embeddings.</br>
                        <br>
                        To start, please run a batch job on Expanse to convert your fasta file to protein sequence embeddings captured in an npz file. If you do not have an account, please create one through <a href="https://portal.xsede.org/my-xsede#/guest">XSEDE User Portal linked here</a>. Once you have an account, you’ll have the option to leverage TAPE or ESM embeddings </br>
                        <br>
                        To run the Application, ensure that you are using python3 server.py
                        </p>
                        <p><h3>About EINSTEIN</h3>
                        <br>
                        Biologists work with a multitude of protein sequences represented by strings of letters. The amino acid sequence of these proteins allows us to leverage various machine learning Natural Language Processing algorithms aimed to predict enzyme classifications which are indicative of both protein structure and functionality. </br>
                        <br>
                        Our goal is to propose a multi level classification solution that is designed to predict the respective class of a given enzyme. Our approach consists of predicting the classification of an enzyme by applying NLP to a protein sequence. Our method utilizes BERT (Bidirectional Encoder Representations from Transformers) models to create embeddings, or feature vectors, and a variety of machine learning models to predict the respective class and subclass of an enzyme.</br>
                        </p>
                    </div>
                </div>
            </div>
            <div class="card">
                <div class="card-header" id="headingTwo">
                    <h2 class="mb-0">
                        <button type="button" class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseTwo">2. About BERT</button>
                    </h2>
                </div>
                <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordionExample">
                    <div class="card-body">
                        <p><h3>TAPE</h3>
                        <br>
                        Researchers at UC Berkeley developed Tasks Assessing Protein Embeddings (TAPE) by pretraining a BERT model on 31 million protein sequences.</br>
                        <br>
                        TAPE was the first attempt at systematically evaluating semi-supervised learning on protein sequences. It includes a set of five biologically relevant supervised tasks that evaluate the performance of learned protein embeddings across diverse aspects of protein understanding. </br>
                        <br>
                        TAPE utilizes semi-supervised learning to jointly leverage information in the unlabeled and labeled data, with the goal of maximizing performance on the supervised task. TAPE utilizes unsupervised pre-trained data of BERT, and provides five biologically relevant downstream prediction tasks to serve as benchmarks. These tasks are categorized into structure prediction, evolutionary understanding, and protein engineering tasks.</br>
                        </p>
                        <p><h3>ESM-1b</h3>
                        <br>
                        Evolutionary Scale Modeling (ESM-1b), developed by researchers at FaceBook, outperforms all tested single-sequence protein language models across a range of structure prediction tasks.</br>
                        <br>
                        The model utilizes unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences. ESM’s Structural Split Dataset is a five-fold cross validation dataset of protein domain structures that can be used to measure generalization of representations across different levels of structural dissimilarity. </br>
                        <br>
                        The representations are learned from sequence data alone, no information about the properties of the sequences is given through supervision or domain-specific features. </br>
                        </p>
                    </div>
                </div>
                <div class="card">
                    <div class="card-header" id="headingThree">
                        <h2 class="mb-0">
                            <button type="button" class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThree">3. Computing Embeddings</button>                     
                        </h2>
                    </div>
                    <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordionExample">
                        <div class="card-body">
                            <p><h3>STEP1: DOWNLOAD TAPE/ESM SCRIPTS (run.py, settings.json, batch.sh) FROM GIT REPO</h3>
                            <p><h3>STEP2: EDIT SCRIPTS TO REFLECT XSEDE USER ACCOUNT</h3>
                            <p><h3>STEP3: RUN SCRIPTS AND EXPORT GENERATED EMBEDDINGS TO LOCAL</h3>
                            <br>
                            <h4>TAPE - to run TAPE embeddings on Expanse, use the below run script</h4>
                             </br>
                            /cm/shared/apps/containers/singularity/tape-0.4-20210427.sif
                            <br>
                            #!/usr/bin/env bash<br>
                            #SBATCH --job-name=tape-0.4-fake_enzyme_test<br>
                            #SBATCH --account=use300<br>
                            #SBATCH --partition=gpu-shared<br>
                            #SBATCH --nodes=1<br>
                            #SBATCH --ntasks-per-node=1<br>
                            #SBATCH --cpus-per-task=10<br>
                            #SBATCH --mem=93G<br>
                            #SBATCH --gpus=1<br>
                            #SBATCH --time=00:30:00<br>
                            #SBATCH --output=%x.o%j.%N<br>
                            <br>
                            declare -xr SINGULARITY_MODULE='singularitypro/3.5'<br>
                            declare -xr SINGULAIRTY_CONTAINER_DIR='/cm/shared/apps/containers/singularity'<br>
                            <br>
                            module purge<br>
                            module load "${SINGULARITY_MODULE}"<br>
                            module list<br>
                            printenv<br>
                            <br>
                            time -p singularity exec --bind /expanse,/scratch --nv "${SINGULAIRTY_CONTAINER_DIR}/ciml/tape-0.4-20210427.sif" python -u run_tape.py</br>
                            </p>
                            <p>
                            <h4>ESM-1b: to run ESM-1b embeddings on Expanse, use the below script</h4>
                            <br>
                            /cm/shared/apps/containers/singularity/esm-0.3.1-20210427.sif
                            <br>
                            #!/usr/bin/env bash
                             <br>
                            #SBATCH --job-name=esm-0.3.1-Enzymes_test_esm <br>
                            #SBATCH --account=use300 <br>
                            #SBATCH --partition=compute <br>
                            #SBATCH --nodes=1 <br>
                            #SBATCH --ntasks-per-node=1 <br>
                            #SBATCH --cpus-per-task=128 <br>
                            #SBATCH --mem=248G <br>
                            #SBATCH --time=00:30:00 <br>
                            #SBATCH --output=%x.o%j.%N <br>
                             <br>
                            declare -xr SINGULARITY_MODULE='singularitypro/3.5' <br>
                            declare -xr SINGULAIRTY_CONTAINER_DIR='/cm/shared/apps/containers/singularity' <br>
                             <br>
                            module purge <br>
                            module load "${SINGULARITY_MODULE}" <br>
                            module list <br>
                            printenv <br>
                            <br>
                            time -p singularity exec --bind /expanse,/scratch "${SINGULAIRTY_CONTAINER_DIR}/ciml/esm-0.3.1-20210427.sif" python -u run_esm.py </br>
                            </p>
                        </div>
                    </div>
                </div>
                <div class="card">
                    <div class="card-header" id="headingThree">
                        <h2 class="mb-0">
                            <button type="button" class="btn btn-link collapsed" data-toggle="collapse" data-target="#collapseThree">4. Exploring Results</button>                     
                        </h2>
                    </div>
                    <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordionExample">
                        <div class="card-body">
                            <p>PLACEHOLDER4</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</body>
</html>                            
